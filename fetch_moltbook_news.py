#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Moltbook ç§‘æŠ€åŠ¨å‘æŠ“å–è„šæœ¬ï¼ˆå¤šé¢‘é“å¢é‡ç‰ˆï¼‰
åŠŸèƒ½ï¼šæ”¯æŒå¤š URL æŠ“å–ã€AI æ±‡æ€»æ€»ç»“ã€å…¨å±€å»é‡å‚¨å­˜è‡³ data.json
"""

import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Tuple
from urllib.parse import urljoin

from duckduckgo_search import DDGS
from playwright.sync_api import sync_playwright
from zoneinfo import ZoneInfo


def get_beijing_time() -> str:
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚"""
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M (åŒ—äº¬æ—¶é—´)")


# å¯¼èˆªç›¸å…³æ–‡æœ¬ï¼ˆæ’é™¤è¿™äº›ï¼Œé¿å…æŠŠå¯¼èˆªå½“æ ‡é¢˜ï¼‰
NAV_TEXT_BLACKLIST = {
    "login", "dashboard", "search", "loading", "moltbook", "beta", "mascot",
    "help", "developers", "privacy", "terms", "owner login", "submolts",
    "notify me", "agree", "receive emails", "built for agents",
}


def summarize_with_ddg(titles: List[str]) -> str:
    """
    ä½¿ç”¨ AI æ±‡æ€»æ€»ç»“å¤šä¸ªé¢‘é“çš„ä»Šæ—¥è¶‹åŠ¿ã€‚
    """
    if not titles:
        return ""

    def _clean_title(t: str) -> str:
        t = re.sub(r"\s+", " ", (t or "").strip())
        t = re.sub(r"^â–²\s*\d+\s*â–¼\s*Posted by\s+u/\S+\s+\S+\s+ago\s+", "", t, flags=re.I)
        t = re.sub(r"\s*ğŸ’¬\s*\d+\s*comments?\s*$", "", t, flags=re.I)
        return t.strip()

    cleaned = [_clean_title(t) for t in titles]
    cleaned = [t for t in cleaned if t]

    def _fallback_three_trends(ts: List[str]) -> str:
        return "- **è·¨é¢†åŸŸæŠ€æœ¯èåˆ**ï¼šå¤šä¸ªé¢‘é“æ˜¾ç¤º AI æ­£åœ¨åŠ é€Ÿå‘å‚ç›´è¡Œä¸šï¼ˆå¦‚é‡‘èã€ç¡¬ä»¶ï¼‰æ¸—é€ã€‚\n- **æ™ºèƒ½ä½“ç”Ÿæ€ååŒ**ï¼šä¸åŒé¢†åŸŸå¯¹å¤šæ™ºèƒ½ä½“åä½œåè®®çš„è®¨è®ºçƒ­åº¦æ˜¾è‘—ä¸Šå‡ã€‚\n- **å·¥ç¨‹åŒ–è½åœ°æé€Ÿ**ï¼šå¼€å‘è€…å…³æ³¨ç‚¹ä»æ¨¡å‹èƒ½åŠ›è½¬å‘ç¨³å®šè¿è¡Œä¸å¤§è§„æ¨¡éƒ¨ç½²ã€‚"

    # AI æ€»ç»“é€šå¸¸å–å‰ 30 æ¡æœ€å…·ä»£è¡¨æ€§çš„
    cleaned = cleaned[:30]
    cleaned = [t[:220] for t in cleaned]

    prompt = (
        "ä½ æ˜¯ç§‘æŠ€èµ„è®¯ç¼–è¾‘ã€‚è¯·åŸºäºä»¥ä¸‹æ±‡æ€»è‡ªå¤šä¸ªé¢‘é“çš„æ ‡é¢˜åˆ—è¡¨ï¼Œç”¨ä¸­æ–‡æ€»ç»“â€œä»Šæ—¥ 3 å¤§è¶‹åŠ¿â€ã€‚\n"
        "è¦æ±‚ï¼šä¸¥æ ¼è¾“å‡º 3 æ¡ï¼›ä½¿ç”¨ Markdown æ— åºåˆ—è¡¨ï¼›æ¯æ¡ 1-2 å¥ï¼›ä¸è¦è¾“å‡ºé¢å¤–å†…å®¹ã€‚\n\n"
        "æ ‡é¢˜åˆ—è¡¨ï¼š\n" + "\n".join(f"- {t}" for t in cleaned)
    )

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    }

    try:
        raw = DDGS(headers=headers, timeout=60).chat(prompt, model="gpt-4o-mini").strip()
        if "-" in raw: return raw
        return _fallback_three_trends(cleaned)
    except:
        return _fallback_three_trends(cleaned)


def scrape_post_links_with_playwright(url: str, base_url: str, item_limit: int) -> List[Tuple[str, str]]:
    """
    æŠ“å–ç‰¹å®š URL çš„é“¾æ¥ã€‚
    """
    results: List[Tuple[str, str]] = []
    seen_urls = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        try:
            page = browser.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            page.wait_for_selector('a[href*="/post/"]', timeout=20000)
            page.wait_for_timeout(2000)

            links = page.query_selector_all('a[href*="/post/"]')
            for link in links:
                href = link.get_attribute("href")
                text = link.inner_text().strip()
                if not href or "/post/" not in href or len(text) < 2: continue
                
                if any(nav in text.lower() for nav in NAV_TEXT_BLACKLIST): continue

                full_url = urljoin(base_url, href)
                if full_url not in seen_urls:
                    seen_urls.add(full_url)
                    results.append((text, full_url))
                    if len(results) >= item_limit: break
        except Exception as e:
            print(f"æŠ“å– {url} å‡ºé”™: {e}")
        finally:
            browser.close()
    return results


def load_config(config_path: Path) -> Tuple[List[str], int]:
    """
    æ ¸å¿ƒä¿®æ”¹ï¼šè¯»å– target_urls (åˆ—è¡¨)ã€‚å¦‚æœä¸å­˜åœ¨åˆ™å…¼å®¹æ—§ç‰ˆ target_urlã€‚
    """
    default_urls = ["https://www.moltbook.com/m/ai"]
    default_limit = 30
    
    if not config_path.exists():
        return default_urls, default_limit

    try:
        data = json.loads(config_path.read_text(encoding="utf-8"))
        # ä¼˜å…ˆè¯»å– target_urls åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¯» target_url å¹¶è½¬ä¸ºåˆ—è¡¨
        urls = data.get("target_urls")
        if not urls:
            single_url = data.get("target_url")
            urls = [single_url] if single_url else default_urls
            
        limit = int(data.get("item_limit", default_limit))
        return urls, limit
    except Exception:
        return default_urls, default_limit


def save_data_incremental(output_path: Path, beijing_time: str, ai_summary: str, new_items: List[Tuple[str, str]]) -> None:
    """
    è¯»å–æ—§æ•°æ®ï¼Œä¸æœ¬æ¬¡æŠ“å–çš„æ‰€æœ‰é¢‘é“å†…å®¹åˆå¹¶å»é‡ã€‚
    """
    existing_items = []
    if output_path.exists():
        try:
            old_data = json.loads(output_path.read_text(encoding="utf-8"))
            existing_items = old_data.get("items", [])
        except:
            pass

    formatted_new = [{"title": t, "url": u} for t, u in new_items]
    combined_list = formatted_new + existing_items
    
    unique_items = []
    seen_urls = set()

    for item in combined_list:
        url = item.get("url")
        if url and url not in seen_urls:
            unique_items.append(item)
            seen_urls.add(url)

    final_items = unique_items[:500]
    payload = {
        "beijing_time": beijing_time,
        "ai_summary": ai_summary,
        "items": final_items,
    }
    output_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"èšåˆå®Œæˆï¼šå…±è®¡ {len(final_items)} æ¡å»é‡æƒ…æŠ¥è®°å½•ã€‚")


def write_readme(items: List[Tuple[str, str]], beijing_time: str, summary_md: str, output_path: Path) -> None:
    lines = [
        "# ğŸ¤– Moltbook ç§‘æŠ€å¤šé¢‘é“ç›‘æµ‹",
        "",
        f"**æ›´æ–°æ—¶é—´ï¼š** {beijing_time}",
        "",
        "## å…¨é¢‘é“è¶‹åŠ¿æ±‡æ€» (AI æ€»ç»“)",
        "",
        summary_md if summary_md.strip() else "- ï¼ˆæš‚æ— æ€»ç»“ï¼‰",
        "",
        "## æœ¬æ¬¡æŠ“å–æ›´æ–°",
        "",
    ]
    if items:
        for i, (title, url) in enumerate(items, 1):
            lines.append(f"{i}. [{title}]({url})")
    else:
        lines.append("*æœ¬æ¬¡æœªå‘ç°æ–°å†…å®¹ã€‚*")
    
    output_path.write_text("\n".join(lines), encoding="utf-8")


def main() -> None:
    script_dir = Path(__file__).resolve().parent
    config_path = script_dir / "config.json"
    data_path = script_dir / "data.json"
    readme_path = script_dir / "README.md"
    
    urls, item_limit = load_config(config_path)
    base_url = "https://www.moltbook.com"
    
    all_new_items = []
    
    # å¾ªç¯æŠ“å–å¤šä¸ªé¢‘é“
    for url in urls:
        print(f"ğŸš€ æ­£åœ¨æŠ“å–é¢‘é“: {url}")
        items = scrape_post_links_with_playwright(url, base_url, item_limit)
        all_new_items.extend(items)
        # ç¤¼è²ŒæŠ“å–ï¼Œé—´éš” 2 ç§’
        time.sleep(2)
    
    print(f"ğŸ“Š æ±‡æ€»å®Œæˆï¼Œå…±æŠ“å–åˆ° {len(all_new_items)} æ¡åŸå§‹æ•°æ®ã€‚å¼€å§‹ AI åˆ†æ...")
    summary = summarize_with_ddg([t for t, _ in all_new_items])
    
    curr_time = get_beijing_time()
    
    # æ‰§è¡Œæ±‡æ€»ä¿å­˜ä¸æ›´æ–°
    save_data_incremental(data_path, curr_time, summary, all_new_items)
    write_readme(all_new_items, curr_time, summary, readme_path)


if __name__ == "__main__":
    main()
