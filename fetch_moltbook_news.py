#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Moltbook ç§‘æŠ€åŠ¨å‘æŠ“å–è„šæœ¬ï¼ˆPlaywright ç‰ˆï¼‰
è®¿é—® https://www.moltbook.com/m/aiï¼Œç­‰å¾… /post/ å†…å®¹åŠ è½½åæŠ“å–æ ‡é¢˜ä¸é“¾æ¥ï¼Œå†™å…¥ README.md
"""

import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Tuple
from urllib.parse import urljoin

from duckduckgo_search import DDGS
from playwright.sync_api import sync_playwright
from zoneinfo import ZoneInfo


def get_beijing_time() -> str:
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚"""
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M (åŒ—äº¬æ—¶é—´)")


# å¯¼èˆªç›¸å…³æ–‡æœ¬ï¼ˆæ’é™¤è¿™äº›ï¼Œé¿å…æŠŠå¯¼èˆªå½“æ ‡é¢˜ï¼‰
NAV_TEXT_BLACKLIST = {
    "login", "dashboard", "search", "loading", "moltbook", "beta", "mascot",
    "help", "developers", "privacy", "terms", "owner login", "submolts",
    "notify me", "agree", "receive emails", "built for agents",
}


def summarize_with_ddg(titles: List[str]) -> str:
    """
    ä½¿ç”¨ duckduckgo_search çš„ DDGS().chat()ï¼ˆæ— éœ€ API Keyï¼‰æ€»ç»“ä»Šæ—¥ 3 å¤§è¶‹åŠ¿ã€‚

    å°†æŠ“å–çš„æ ‡é¢˜ä¼ å…¥ï¼Œå›ºå®šä½¿ç”¨ 'gpt-4o-mini' æ¨¡å‹ï¼Œè¿”å› Markdown åˆ—è¡¨æ–‡æœ¬ã€‚
    """
    if not titles:
        return ""

    def _clean_title(t: str) -> str:
        t = re.sub(r"\s+", " ", (t or "").strip())
        # å»æ‰ç±»ä¼¼ Reddit é£æ ¼çš„å™ªéŸ³å‰ç¼€/åç¼€ï¼ˆä¸å½±å“ README ä¸­åŸå§‹æ ‡é¢˜å±•ç¤ºï¼‰
        t = re.sub(r"^â–²\s*\d+\s*â–¼\s*Posted by\s+u/\S+\s+\S+\s+ago\s+", "", t, flags=re.I)
        t = re.sub(r"\s*ğŸ’¬\s*\d+\s*comments?\s*$", "", t, flags=re.I)
        return t.strip()

    cleaned = [_clean_title(t) for t in titles]
    cleaned = [t for t in cleaned if t]

    def _fallback_three_trends(ts: List[str]) -> str:
        tl = " ".join(t.lower() for t in ts)
        themes = [
            (
                "AI ä»£ç†æ¡†æ¶çš„å·¥ç¨‹åŒ–ä¸è½åœ°æŒ‘æˆ˜",
                ["framework", "agent framework", "limitations", "production", "demo", "edge case", "error handling", "prod"],
                "è®¨è®ºä» Demo èµ°å‘ç”Ÿäº§çš„é¸¿æ²Ÿï¼šç¨³å®šæ€§ã€è¾¹ç•Œæ¡ä»¶ã€é”™è¯¯å¤„ç†ä¸å¯ç»´æŠ¤æ€§æˆä¸ºæ ¸å¿ƒã€‚",
            ),
            (
                "å¤šæ™ºèƒ½ä½“åä½œã€é€šä¿¡ä¸è®°å¿†åŸºç¡€è®¾æ–½",
                ["multi-agent", "agent-to-agent", "coordination", "communication", "bridge", "memory", "shared memory", "pheromone", "colony", "protocol"],
                "å›´ç»•å¤šæ™ºèƒ½ä½“åä½œçš„é€šä¿¡åè®®ã€å…±äº«è®°å¿†ä¸ç¾¤ä½“åè°ƒæœºåˆ¶çš„æ¢ç´¢æ˜æ˜¾å¢å¤šã€‚",
            ),
            (
                "è‡ªæ²»ä¸ä¿¡ä»»ï¼šä»£ç†å¦‚ä½•åœ¨é€‚å½“æ—¶æœºè¡ŒåŠ¨/ä¸è¡ŒåŠ¨",
                ["autonomy", "trust", "permission", "act", "useful", "wait", "value", "decision"],
                "å…³æ³¨ä»£ç†çš„è‡ªæ²»è¾¹ç•Œä¸äººæœºä¿¡ä»»å…³ç³»ï¼šä½•æ—¶ä¸»åŠ¨ã€ä½•æ—¶å…‹åˆ¶ï¼Œç›´æ¥å½±å“é•¿æœŸå¯ç”¨æ€§ã€‚",
            ),
            (
                "API åŒ–ã€ç»“æ„åŒ–æ•°æ®ä¸åŸºç¡€è®¾æ–½æ€ç»´",
                ["api", "endpoint", "json", "shell", "infrastructure"],
                "æ›´åå‘ç”¨ API/ç»“æ„åŒ–æ•°æ®ç›´è¿ç³»ç»Ÿï¼Œå¼ºè°ƒâ€œå¯ç»„åˆâ€çš„åŸºç¡€è®¾æ–½è€Œéç•Œé¢å±‚è¡¨è±¡ã€‚",
            ),
            (
                "é‡åŒ–/åŠ å¯†é£é™©ç®¡ç†ä¸ä»“ä½æ•°å­¦",
                ["kelly", "crypto", "position sizing", "portfolio", "trade"],
                "å°‘é‡å†…å®¹èšç„¦äº¤æ˜“é£é™©æ§åˆ¶ä¸ä»“ä½ç®¡ç†ï¼Œç”¨æ•°å­¦çº¦æŸæ³¢åŠ¨ä¸å›æ’¤ã€‚",
            ),
        ]

        scored = []
        for name, keys, desc in themes:
            score = sum(1 for k in keys if k in tl)
            scored.append((score, name, desc))
        scored.sort(key=lambda x: x[0], reverse=True)

        picked = [x for x in scored if x[0] > 0][:3]
        if len(picked) < 3:
            # å…œåº•è¡¥é½ 3 æ¡
            for x in scored:
                if x not in picked:
                    picked.append(x)
                if len(picked) >= 3:
                    break

        return "\n".join(f"- **{name}**ï¼š{desc}" for _, name, desc in picked[:3])

    # æ§åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…è§¦å‘ä¼šè¯/é•¿åº¦é™åˆ¶/é™æµ
    cleaned = cleaned[:25]
    cleaned = [t[:220] for t in cleaned]

    prompt = (
        "ä½ æ˜¯ç§‘æŠ€èµ„è®¯ç¼–è¾‘ã€‚è¯·åŸºäºä»¥ä¸‹æ ‡é¢˜åˆ—è¡¨ï¼Œç”¨ä¸­æ–‡æ€»ç»“â€œä»Šæ—¥ 3 å¤§è¶‹åŠ¿â€ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) ä¸¥æ ¼è¾“å‡º 3 æ¡ï¼›\n"
        "2) ä½¿ç”¨ Markdown æ— åºåˆ—è¡¨ï¼ˆæ¯æ¡ä»¥ - å¼€å¤´ï¼‰ï¼›\n"
        "3) æ¯æ¡ 1-2 å¥ï¼Œæç‚¼ä¸»é¢˜ï¼Œä¸è¦å¤è¿°ç‚¹èµ/ä½œè€…/è¯„è®ºæ•°ç­‰å™ªéŸ³ï¼›\n"
        "4) ä¸è¦è¾“å‡ºé™¤è¿™ 3 æ¡ä»¥å¤–çš„ä»»ä½•å†…å®¹ã€‚\n\n"
        "æ ‡é¢˜åˆ—è¡¨ï¼š\n"
        + "\n".join(f"- {t}" for t in cleaned)
    )

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }

    last_err = None
    for attempt in range(3):
        try:
            raw = DDGS(headers=headers, timeout=60).chat(prompt, model="gpt-4o-mini", timeout=60).strip()
            lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
            bullets = []
            for ln in lines:
                if ln.startswith(("-", "â€¢")):
                    bullets.append("- " + ln.lstrip("-â€¢").strip())
                elif re.match(r"^\d+[.)]\s+", ln):
                    bullets.append("- " + re.sub(r"^\d+[.)]\s+", "", ln).strip())
            bullets = bullets[:3]
            if len(bullets) == 3:
                return "\n".join(bullets)
            # è¾“å‡ºä¸ç¬¦åˆè¦æ±‚åˆ™èµ°æœ¬åœ°å…œåº•
            return _fallback_three_trends(cleaned)
        except Exception as e:
            last_err = e
            # 418/é™æµæ—¶åšç®€å•é€€é¿é‡è¯•
            time.sleep(1 + attempt * 2)

    # å¤šæ¬¡å¤±è´¥ï¼šç»™å‡ºæœ¬åœ°å…œåº•ï¼ˆä»ä¿è¯ 3 æ¡ï¼‰
    _ = last_err  # ä»…ä¿ç•™ä»¥ä¾¿æœªæ¥è°ƒè¯•
    return _fallback_three_trends(cleaned)


def scrape_post_links_with_playwright(url: str, base_url: str, item_limit: int) -> List[Tuple[str, str]]:
    """
    ä½¿ç”¨ Playwright æ‰“å¼€é¡µé¢ï¼Œç­‰å¾…å¸¦ /post/ çš„é“¾æ¥å‡ºç°ï¼ŒæŠ“å–æ‰€æœ‰æ ‡é¢˜å’Œé“¾æ¥ã€‚
    è¿‡æ»¤é‡å¤ä¸å¯¼èˆªé“¾æ¥ã€‚
    """
    results: List[Tuple[str, str]] = []
    seen_urls: set = set()
    seen_titles_norm: set = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        try:
            page = browser.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=30000)

            # ç­‰å¾…å¸¦æœ‰ /post/ çš„é“¾æ¥å‡ºç°ï¼ˆæœ€å¤šç­‰ 20 ç§’ï¼‰
            page.wait_for_selector('a[href*="/post/"]', timeout=20000)

            # å†ç»™ä¸€ç‚¹æ—¶é—´è®©åˆ—è¡¨ç¨³å®š
            page.wait_for_timeout(2000)

            # è·å–æ‰€æœ‰åŒ…å« /post/ çš„é“¾æ¥
            links = page.query_selector_all('a[href*="/post/"]')

            for link in links:
                href = link.get_attribute("href")
                if not href or "/post/" not in href:
                    continue

                text = link.inner_text().strip()
                text = re.sub(r"\s+", " ", text)

                # è¿‡æ»¤ç©ºæ ‡é¢˜æˆ–è¿‡çŸ­
                if not text or len(text) < 2:
                    continue

                # æ’é™¤å¯¼èˆªç±»æ–‡æœ¬
                text_lower = text.lower()
                if any(nav in text_lower for nav in NAV_TEXT_BLACKLIST):
                    continue

                full_url = urljoin(base_url, href)
                # å»é‡ï¼šæŒ‰ URL
                if full_url in seen_urls:
                    continue
                seen_urls.add(full_url)

                # å»é‡ï¼šæŒ‰è§„èŒƒåŒ–æ ‡é¢˜ï¼ˆé¿å…åŒä¸€æ–‡ç« ä¸åŒæ ¼å¼é‡å¤ï¼‰
                title_norm = text.strip().lower()[:80]
                if title_norm in seen_titles_norm:
                    continue
                seen_titles_norm.add(title_norm)

                results.append((text.strip(), full_url))
                if len(results) >= item_limit:
                    break

        except Exception as e:
            print(f"æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            browser.close()

    return results


def load_config(config_path: Path) -> Tuple[str, int]:
    """
    è¯»å– config.jsonï¼Œè·å– target_url ä¸ item_limitã€‚
    æ–‡ä»¶ä¸å­˜åœ¨æˆ–å­—æ®µç¼ºå¤±/éæ³•æ—¶ä½¿ç”¨é»˜è®¤å€¼ã€‚
    """
    default_url = "https://www.moltbook.com/m/ai"
    default_limit = 30

    if not config_path.exists():
        return default_url, default_limit

    try:
        data = json.loads(config_path.read_text(encoding="utf-8"))
    except Exception:
        return default_url, default_limit

    target_url = data.get("target_url", default_url)
    if not isinstance(target_url, str) or not target_url.strip():
        target_url = default_url

    item_limit = data.get("item_limit", default_limit)
    try:
        item_limit_int = int(item_limit)
    except Exception:
        item_limit_int = default_limit

    # åˆç†èŒƒå›´ä¿æŠ¤
    item_limit_int = max(1, min(item_limit_int, 200))
    return target_url, item_limit_int


def save_data_json(output_path: Path, beijing_time: str, ai_summary: str, items: List[Tuple[str, str]]) -> None:
    payload = {
        "beijing_time": beijing_time,
        "ai_summary": ai_summary,
        "items": [{"title": t, "url": u} for t, u in items],
    }
    output_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def write_readme(items: List[Tuple[str, str]], beijing_time: str, summary_md: str, output_path: Path) -> None:
    """å°†æ ‡é¢˜ã€æ—¶é—´å’Œæ¡ç›®åˆ—è¡¨å†™å…¥ README.mdã€‚"""
    lines = [
        "# ğŸ¤– Moltbook ç§‘æŠ€åŠ¨å‘è‡ªåŠ¨ç›‘æµ‹",
        "",
        f"**æ›´æ–°æ—¶é—´ï¼š** {beijing_time}",
        "",
        "## ä»Šæ—¥ 3 å¤§è¶‹åŠ¿ï¼ˆDuckDuckGo AI æ€»ç»“ï¼‰",
        "",
    ]

    if summary_md and summary_md.strip():
        lines.extend(summary_md.strip().splitlines())
    else:
        lines.append("- ï¼ˆæš‚æ— æ€»ç»“ï¼‰")

    lines.extend(
        [
            "",
            "## æœ€æ–°åŠ¨å‘",
            "",
        ]
    )
    if items:
        for i, (title, url) in enumerate(items, 1):
            lines.append(f"{i}. [{title}]({url})")
        lines.append("")
    else:
        lines.append("*æš‚æ— è§£æåˆ°å¸¦é“¾æ¥çš„æ¡ç›®ï¼ˆå¯èƒ½è¶…æ—¶æˆ–é¡µé¢æ—  /post/ å†…å®¹ï¼‰ã€‚*")
        lines.append("")

    output_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"å·²å†™å…¥: {output_path.absolute()}")


def main() -> None:
    script_dir = Path(__file__).resolve().parent
    config_path = script_dir / "config.json"
    url, item_limit = load_config(config_path)
    base_url = "https://www.moltbook.com"
    output_path = script_dir / "README.md"
    data_path = script_dir / "data.json"

    print("æ­£åœ¨ä½¿ç”¨ Playwright æŠ“å–é¡µé¢...")
    items = scrape_post_links_with_playwright(url, base_url, item_limit=item_limit)

    titles = [t for t, _ in items]
    print("æ­£åœ¨ç”Ÿæˆ DuckDuckGo AI æ€»ç»“...")
    summary_md = summarize_with_ddg(titles)

    beijing_time = get_beijing_time()
    write_readme(items, beijing_time, summary_md, output_path)
    save_data_json(data_path, beijing_time, summary_md, items)


if __name__ == "__main__":
    main()
