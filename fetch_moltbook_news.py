#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """å¢é‡åˆ†æ‰¹ç¿»è¯‘ï¼šé™ä½å•æ¬¡è°ƒç”¨å‹åŠ›ï¼Œæé«˜ç¨³å®šæ€§"""
    if not client or not new_items: return new_items

    trans_map = {it["url"]: it["title_cn"] for it in existing_items if "title_cn" in it}
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate:
        print("â˜• æ²¡æœ‰æ–°æ–‡ç« éœ€è¦ç¿»è¯‘ã€‚")
        return new_items

    print(f"ğŸŒ å‘ç° {len(to_translate)} æ¡æ–°å†…å®¹ï¼Œå¼€å§‹åˆ†æ‰¹ç¿»è¯‘...")
    
    # æ¯ 10 æ¡ä¸ºä¸€ç»„è¿›è¡Œç¿»è¯‘ï¼Œé˜²æ­¢ AI å¡æ­»
    chunk_size = 10
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        print(f"æ­£åœ¨ç¿»è¯‘ç¬¬ {i+1} åˆ° {i+len(chunk)} æ¡...")
        
        prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘æŠ€ç¿»è¯‘ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚åªè¦è¾“å‡ºç¿»è¯‘ï¼Œä¸€è¡Œä¸€ä¸ªï¼š\n\n" + \
                 "\n".join([it["title"] for it in chunk])
        
        try:
            completion = client.chat.completions.create(
                model="qwen-plus",
                messages=[{"role": "user", "content": prompt}],
                timeout=30 # è®¾ç½®å•æ¬¡è¯·æ±‚è¶…æ—¶
            )
            res = completion.choices[0].message.content.strip().splitlines()
            for j, it in enumerate(chunk):
                if j < len(res):
                    it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[j].strip())
                else:
                    it["title_cn"] = it["title"]
        except Exception as e:
            print(f"âŒ è¯¥æ‰¹æ¬¡ç¿»è¯‘å¤±è´¥: {e}")
            for it in chunk: it["title_cn"] = it.get("title_cn", it["title"])
        
        time.sleep(0.5) # å¾®å°é—´è·ï¼Œé˜²æ­¢é¢‘ç‡è¿‡é«˜

    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return ""
    titles = [it.get("title_cn", it["title"]) for it in items[:40]]
    prompt = "åŸºäºä»¥ä¸‹æ ‡é¢˜ï¼Œæ€»ç»“ä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ã€10æ¡ã€Markdownåˆ—è¡¨ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(f"- {t}" for t in titles)

    try:
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
        return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ APIï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    """å¤ç”¨æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œæé€ŸæŠ“å–å¤šé¢‘é“"""
    from playwright.sync_api import sync_playwright
    all_results = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å– {cat}...")
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=25000)
                page.wait_for_selector('a[href*="/post/"]', timeout=15000)
                
                links = page.query_selector_all('a[href*="/post/"]')
                count = 0
                for link in links:
                    href = link.get_attribute("href")
                    text = link.inner_text().strip()
                    if not href or "/post/" not in href or len(text) < 5: continue
                    
                    all_results.append({
                        "title": text,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat
                    })
                    count += 1
                    if count >= limit: break
                page.close()
                print(f"âœ… {cat} æŠ“å–å®Œæˆï¼Œè·å– {count} æ¡ã€‚")
            except Exception as e:
                print(f"âŒ {cat} è®¿é—®è¶…æ—¶æˆ–å‡ºé”™ï¼Œè·³è¿‡ã€‚")
        
        browser.close()
    return all_results

def main():
    script_dir = Path(__file__).resolve().parent
    data_path = script_dir / "data.json"
    
    # åŠ è½½é…ç½®
    config = json.loads((script_dir / "config.json").read_text())
    urls = config.get("target_urls", [])
    limit = config.get("item_limit", 19)

    # 1. æŠ“å– (å¤ç”¨æµè§ˆå™¨)
    all_new = scrape_all_channels(urls, limit)

    # 2. è¯»å–æ—§æ•°æ®
    existing_items = []
    if data_path.exists():
        try: existing_items = json.loads(data_path.read_text(encoding="utf-8")).get("items", [])
        except: pass

    # 3. å¢é‡ç¿»è¯‘ + æ€»ç»“
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    # 4. å»é‡åˆå¹¶ (ä¿ç•™ 500 æ¡)
    combined = all_new + existing_items
    unique = []
    seen = set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it)
            seen.add(it["url"])

    # 5. ä¿å­˜
    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡å®Œç¾ç»“æŸï¼")

if __name__ == "__main__":
    main()
