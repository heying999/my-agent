#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict # ç¡®ä¿å¯¼å…¥
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """å¦‚æœåˆ é™¤äº† data.jsonï¼Œè¿™é‡Œä¼šå…¨é‡ç¿»è¯‘å‰ 30 æ¡"""
    if not client or not new_items: return new_items
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn")}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate: return new_items

    # åˆšé‡ç½®æ—¶ï¼Œæ–‡ç« å¾ˆå¤šï¼Œæˆ‘ä»¬å…ˆç¿»è¯‘æœ€å‰é¢çš„ 30 æ¡ï¼Œå‰©ä¸‹çš„ä»¥åæ…¢æ…¢ç¿»
    max_batch = 30
    process_list = to_translate[:max_batch]
    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(process_list)} æ¡æ–°é¢˜ç›®...", flush=True)
    
    chunk_size = 10
    for i in range(0, len(process_list), chunk_size):
        chunk = process_list[i : i + chunk_size]
        prompt = "å°†ä»¥ä¸‹ç§‘æŠ€æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¦è¾“å‡ºç¿»è¯‘ï¼Œä¸€è¡Œä¸€ä¸ªï¼š\n\n" + "\n".join([it["title"] for it in chunk])
        try:
            completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
            res = completion.choices[0].message.content.strip().splitlines()
            for j, it in enumerate(chunk):
                if j < len(res):
                    it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[j].strip())
                else: it["title_cn"] = it["title"]
        except Exception as e: print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}", flush=True)
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return ""
    # æ€»ç»“ä½¿ç”¨ç¿»è¯‘åçš„æ ‡é¢˜
    titles = [it.get("title_cn", it["title"]) for it in items[:30]]
    prompt = "æ€»ç»“ä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ã€10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(f"- {t}" for t in titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except: return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å–: {cat}...", flush=True)
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                # ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼šåªè¦æ˜¯åŒ…å« /post/ çš„é“¾æ¥
                page.wait_for_selector('a[href*="/post/"]', timeout=20000)
                
                # æŠ“å–æ‰€æœ‰æ–‡ç« é“¾æ¥
                links = page.query_selector_all('a[href*="/post/"]')
                count = 0
                seen_urls = set()
                
                for link in links:
                    href = link.get_attribute("href")
                    if not href or href in seen_urls: continue
                    
                    # è·å–è¯¥é“¾æ¥æ‰€åœ¨çš„å®¹å™¨æ–‡å­—ï¼Œç”¨æ¥æå–çƒ­åº¦
                    # å‘ä¸Šæ‰¾ä¸¤å±‚é€šå¸¸èƒ½è¦†ç›–æ•´ä¸ªå¡ç‰‡
                    parent = link.evaluate_handle("el => el.parentElement.parentElement")
                    raw_text = parent.as_element().inner_text() if parent.as_element() else ""
                    
                    # æ¸…æ´—é¢˜ç›®ï¼šå¦‚æœæ˜¯é‚£ç§åŒ…å«èµæ•°çš„æ–‡å­—ï¼Œåªå–é¢˜ç›®éƒ¨åˆ†
                    title = link.inner_text().strip()
                    if not title or len(title) < 10 or "comments" in title.lower(): continue

                    # æå–èµæ•°å’Œè¯„è®ºæ•°
                    score = re.search(r'[â–²\^]\s*(\d+)', raw_text)
                    comments = re.search(r'(\d+)\s*comments', raw_text.lower())
                    score_val = score.group(1) if score else "0"
                    comment_val = comments.group(1) if comments else "0"

                    all_results.append({
                        "title": title.split('\n')[0], # åªè¦ç¬¬ä¸€è¡Œ
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "hot_info": f"ğŸ”¥{score_val} Â· ğŸ’¬{comment_val}"
                    })
                    seen_urls.add(href)
                    count += 1
                    if count >= limit: break
                
                print(f"âœ… {cat} æŠ“å–åˆ° {count} æ¡ã€‚", flush=True)
                page.close()
            except Exception as e: print(f"âŒ {cat} æŠ“å–è¶…æ—¶æˆ–é”™è¯¯: {e}", flush=True)
        browser.close()
    return all_results

def main():
    script_dir = Path(__file__).resolve().parent
    data_path = script_dir / "data.json"
    
    # è¯»å–é…ç½®
    try:
        config = json.loads((script_dir / "config.json").read_text())
        urls = config.get("target_urls", [])
        limit = config.get("item_limit", 19)
    except:
        urls = ["https://www.moltbook.com/m/ai"]; limit = 19

    # 1. æŠ“å–
    all_new = scrape_all_channels(urls, limit)
    if not all_new:
        print("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥ç½‘å€æˆ–é€‰æ‹©å™¨ã€‚", flush=True)

    # 2. è¯»å–æ—§æ•°æ®ï¼ˆå¦‚æœå·²åˆ é™¤åˆ™ä¸ºç©ºï¼‰
    existing_items = []
    if data_path.exists():
        try: existing_items = json.loads(data_path.read_text(encoding="utf-8")).get("items", [])
        except: pass

    # 3. ç¿»è¯‘ä¸æ€»ç»“
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    # 4. å»é‡åˆå¹¶
    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it); seen.add(it["url"])

    # 5. ä¿å­˜å› data.json
    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ æˆåŠŸï¼å½“å‰ data.json å…±æœ‰ {len(unique[:500])} æ¡æƒ…æŠ¥ã€‚", flush=True)

if __name__ == "__main__":
    main()
