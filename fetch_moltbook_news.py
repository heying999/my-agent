import os
import re
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: 
        print("âš ï¸ ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®")
        return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    if not client or not new_items: return new_items
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn") and len(it["title_cn"]) > 1}
    to_translate = [it for it in new_items if it["url"] not in trans_map]

    if not to_translate: 
        print("âœ… æ— éœ€ç¿»è¯‘æ–°æ¡ç›®ã€‚")
        for it in new_items: it["title_cn"] = trans_map.get(it["url"], "")
        return new_items

    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(to_translate)} æ¡æ–°é¢˜ç›®...", flush=True)
    chunk_size = 10 
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
            "è§„åˆ™ï¼šä¸¥æ ¼ä¸€è¡Œå¯¹åº”ä¸€ä¸ªï¼Œä¿æŒé¡ºåºï¼Œä¸¥ç¦è§£é‡Šã€‚\n\n"
            + "\n".join([f"[{idx}] {it['title']}" for idx, it in enumerate(chunk)])
        )
        try:
            completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}], temperature=0.1)
            res = completion.choices[0].message.content.strip().splitlines()
            cleaned = [re.sub(r'^\[\d+\]\s*', '', l).strip() for l in res if l.strip()]
            for j, it in enumerate(chunk):
                it["title_cn"] = cleaned[j] if j < len(cleaned) else it["title"]
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            for it in chunk: it["title_cn"] = it["title"]
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return "æš‚æ— æ‘˜è¦"
    titles = [it.get("title_cn") or it["title"] for it in items[:25]]
    prompt = "ç”¨ç®€ä½“ä¸­æ–‡æ€»ç»“ä»Šæ—¥ AI/ç§‘æŠ€ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except: return "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    
    with sync_playwright() as p:
        print("ğŸ”¥ å¯åŠ¨å·¥ä¸šçº§é‡‡é›†å¼•æ“...", flush=True)
        # å¢åŠ å‚æ•°ï¼šç¦ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾ï¼Œé˜²æ­¢è¢«ç½‘ç«™æ‹¦æˆª
        browser = p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-blink-features=AutomationControlled'])
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸ“¡ æ­£åœ¨æ”»å…‹é¢‘é“: {cat}", flush=True)
            page = context.new_page()
            
            # æ³¨å…¥è„šæœ¬éšè— Playwright ç‰¹å¾
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            try:
                # ç­–ç•¥ 1: ä¸è¦ç­‰ networkidleï¼ˆå®¹æ˜“å¡æ­»ï¼‰ï¼Œç­‰ domcontentloaded åé…åˆæ‰‹åŠ¨ç¡¬ç­‰å¾…
                page.goto(url, wait_until="domcontentloaded", timeout=60000)
                
                # å¼ºè¡Œç­‰å¾… 8 ç§’ï¼Œç¡®ä¿æ‰€æœ‰å¼‚æ­¥æ•°æ®ï¼ˆAJAXï¼‰æ¸²æŸ“å®Œæ¯•
                time.sleep(8) 
                
                # ç­–ç•¥ 2: é¢„è§ˆå†…å®¹ã€‚å¦‚æœåªçœ‹åˆ° "Checking your browser"ï¼Œè¯´æ˜è¢«é˜²ç«å¢™æŒ¡äº†
                body_preview = page.inner_text('body')[:150].replace('\n', ' ')
                print(f"ğŸ‘€ é¡µé¢å¿«ç…§é¢„è§ˆ: {body_preview}", flush=True)

                # ç­–ç•¥ 3: è¯­ä¹‰åŒ–æŠ“å–ã€‚ç›´æ¥æ‰¾æ‰€æœ‰åŒ…å« '/post/' çš„ A æ ‡ç­¾
                # è¿™æ¯”æ‰¾ div ç±»åè¦ç¨³å®š 100 å€
                post_links = page.locator('a[href*="/post/"]')
                link_count = post_links.count()
                print(f"ğŸ” æ‰«æåˆ° {link_count} ä¸ªæ–‡ç« å€™é€‰é“¾æ¥", flush=True)

                count = 0
                seen_urls = set()
                for i in range(link_count):
                    if count >= limit: break
                    link_el = post_links.nth(i)
                    
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title = link_el.inner_text().strip()
                    href = link_el.get_attribute("href")
                    
                    # è¿‡æ»¤å™ªéŸ³ï¼šå¤ªçŸ­çš„æ ‡é¢˜æˆ–é‡å¤çš„é“¾æ¥
                    if not title or len(title) < 10 or href in seen_urls:
                        continue
                        
                    full_url = urljoin("https://www.moltbook.com", href)
                    seen_urls.add(href)
                    
                    # å°è¯•å¯»æ‰¾ç‚¹èµæ•°ï¼ˆé€šå¸¸å°±åœ¨ A æ ‡ç­¾é™„è¿‘ï¼‰
                    all_results.append({
                        "title": title,
                        "url": full_url,
                        "category": cat,
                        "title_cn": "",
                        "hot_info": "ğŸ”¥ çƒ­é—¨å†…å®¹"
                    })
                    count += 1
                
                print(f"âœ… {cat} æŠ“å–æˆåŠŸï¼Œæ•è· {count} æ¡å†…å®¹", flush=True)

            except Exception as e:
                print(f"âŒ {cat} é¢‘é“æŠ“å–å¼‚å¸¸: {e}", flush=True)
            finally:
                page.close()
        browser.close()
    return all_results

def main():
    print(f"ğŸ¬ æœºå™¨äººå¯åŠ¨ | åŒ—äº¬æ—¶é—´: {get_beijing_time()}", flush=True)
    data_path = Path("data.json")
    config_path = Path("config.json")

    if not config_path.exists():
        print("âŒ é”™è¯¯: æ ¹ç›®å½•ç¼ºå°‘ config.json")
        return

    config = json.loads(config_path.read_text())
    # 1. æ‰§è¡ŒæŠ“å–
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 15))
    
    if not all_new:
        print("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•å†…å®¹ã€‚è¯·æ£€æŸ¥ GitHub æ—¥å¿—ä¸­çš„'é¡µé¢å¿«ç…§é¢„è§ˆ'ã€‚")

    # 2. è¯»å–æ—§æ•°æ®
    existing_items = []
    if data_path.exists():
        try:
            old_data = json.loads(data_path.read_text(encoding="utf-8"))
            existing_items = old_data.get("items", [])
        except: pass

    # 3. ç¿»è¯‘ & æ€»ç»“
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new + existing_items, client)

    # 4. åˆå¹¶å»é‡
    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it)
            seen.add(it["url"])

    # 5. å†™å…¥
    output = {
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }
    
    data_path.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼å½“å‰åº“å­˜: {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
