#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """å¢é‡ç¿»è¯‘ï¼šåªç¿»è¯‘åº“é‡Œæ²¡æœ‰çš„æ–°æ ‡é¢˜"""
    if not client or not new_items: return new_items

    # 1. å»ºç«‹æ—§ç¿»è¯‘ç´¢å¼• {url: title_cn}
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if "title_cn" in it}
    
    # 2. è¯†åˆ«éœ€è¦æ–°ç¿»è¯‘çš„æ¡ç›®
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate:
        print("â˜• æ‰€æœ‰æ–‡ç« å‡å·²ç¿»è¯‘è¿‡ï¼Œè·³è¿‡ API è°ƒç”¨ã€‚")
        return new_items

    # 3. æ‰¹é‡ç¿»è¯‘æ–°æ¡ç›®
    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(to_translate)} æ¡æ–°å‘ç°çš„æƒ…æŠ¥...")
    prompt = "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ç¿»è¯‘ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚è¦æ±‚å‡†ç¡®ä¸“ä¸šï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªç¿»è¯‘ï¼Œä¸è¦è¾“å‡ºåºå·å’Œå¤šä½™æ–‡å­—ï¼š\n\n" + \
             "\n".join([it["title"] for it in to_translate])
    
    try:
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}]
        )
        res = completion.choices[0].message.content.strip().splitlines()
        for i, it in enumerate(to_translate):
            if i < len(res):
                it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[i].strip())
            else:
                it["title_cn"] = it["title"]
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        for it in to_translate: it["title_cn"] = it["title"]
    
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    """ç”Ÿæˆ 10 å¤§æ ¸å¿ƒåŠ¨å‘æ€»ç»“"""
    if not client or not items: return ""
    
    # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡æ ‡é¢˜è¿›è¡Œæ€»ç»“ï¼Œæ›´å‡†ç¡®
    titles = [it.get("title_cn", it["title"]) for it in items[:40]]
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€æ–°é—»ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ ‡é¢˜ï¼Œç”¨ã€ç®€ä½“ä¸­æ–‡ã€‘æ€»ç»“â€œä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘â€ã€‚\n"
        "è¦æ±‚ï¼šä¸¥æ ¼ 10 æ¡ï¼›Markdown åˆ—è¡¨ï¼›æ¯æ¡ 1 å¥ç®€æï¼›åŠ ç²—æ ¸å¿ƒå…³é”®è¯ï¼›ã€ä¸¥ç¦è¾“å‡ºè‹±æ–‡ã€‘ã€‚\n\n"
        "æ ‡é¢˜åˆ—è¡¨ï¼š\n" + "\n".join(f"- {t}" for t in titles)
    )

    try:
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ€»ç»“å¤±è´¥: {e}")
        return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API çŠ¶æ€ï¼‰"

def scrape_channel(url: str, limit: int) -> List[Dict]:
    results = []
    cat = url.split('/')[-1].upper()
    from playwright.sync_api import sync_playwright
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        try:
            page = browser.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            page.wait_for_selector('a[href*="/post/"]', timeout=20000)
            links = page.query_selector_all('a[href*="/post/"]')
            for link in links:
                href = link.get_attribute("href")
                text = link.inner_text().strip()
                if not href or "/post/" not in href or len(text) < 2: continue
                results.append({
                    "title": text,
                    "url": urljoin("https://www.moltbook.com", href),
                    "category": cat
                })
                if len(results) >= limit: break
        except Exception as e: print(f"æŠ“å– {cat} å¤±è´¥: {e}")
        finally: browser.close()
    return results

def main():
    script_dir = Path(__file__).resolve().parent
    config = json.loads((script_dir / "config.json").read_text())
    urls = config.get("target_urls", [])
    limit = config.get("item_limit", 20)
    
    # åŠ è½½æ—§æ•°æ®
    data_path = script_dir / "data.json"
    existing_data = {}
    if data_path.exists():
        try: existing_data = json.loads(data_path.read_text(encoding="utf-8"))
        except: pass
    existing_items = existing_data.get("items", [])

    # æŠ“å–æ–°å†…å®¹
    all_new = []
    for url in urls:
        all_new.extend(scrape_channel(url, limit))
        time.sleep(1)

    # å¢é‡å¤„ç†
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    # å»é‡å¹¶ä¿å­˜
    combined = all_new + existing_items
    unique = []
    seen = set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it)
            seen.add(it["url"])

    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… ä»»åŠ¡å®Œæˆï¼Œå½“å‰åº“å­˜ {len(unique[:500])} æ¡")

if __name__ == "__main__":
    main()
