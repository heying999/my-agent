import os
import re
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: 
        print("âš ï¸ ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®")
        return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    if not client or not new_items: return new_items
    
    # å»ºç«‹ç¼“å­˜æ˜ å°„
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn") and len(it["title_cn"]) > 1}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)

    if not to_translate: 
        print("âœ… æ‰€æœ‰æ¡ç›®å‡å·²æœ‰ç¿»è¯‘ï¼Œè·³è¿‡ API è°ƒç”¨ã€‚")
        return new_items

    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(to_translate)} æ¡æ–°é¢˜ç›®...", flush=True)
    chunk_size = 10 
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
            "è§„åˆ™ï¼šä¸¥æ ¼ä¸€è¡Œå¯¹åº”ä¸€ä¸ªï¼Œä¸¥ç¦ä»»ä½•è§£é‡Šã€‚ä¿æŒé¡ºåºä¸€è‡´ã€‚\n\n"
            + "\n".join([f"[{idx}] {it['title']}" for idx, it in enumerate(chunk)])
        )
        try:
            completion = client.chat.completions.create(
                model="qwen-plus", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            raw_res = completion.choices[0].message.content.strip().splitlines()
            cleaned_res = [re.sub(r'^\[\d+\]\s*', '', line).strip() for line in raw_res if line.strip()]
            
            for j, it in enumerate(chunk):
                it["title_cn"] = cleaned_res[j] if j < len(cleaned_res) else it["title"]
        except Exception as e:
            print(f"âŒ ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: {e}")
            for it in chunk: it["title_cn"] = it["title"]
            
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return "æš‚æ— æ‘˜è¦"
    # å–å‰20æ¡è¿›è¡Œæ€»ç»“
    titles = [it.get("title_cn") or it["title"] for it in items[:20]]
    prompt = "æ€»ç»“ä»Šæ—¥ AI ä¸ç§‘æŠ€ 10 å¤§åŠ¨å‘ã€‚ç®€ä½“ä¸­æ–‡ã€10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except:
        return "ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    with sync_playwright() as p:
        print("ğŸ”¥ å¯åŠ¨æµè§ˆå™¨...", flush=True)
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å–é¢‘é“: {cat}...", flush=True)
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=40000)
                page.wait_for_selector('div.flex.flex-col.gap-1', timeout=20000)
                
                cards = page.query_selector_all('div.flex.flex-col.gap-1')
                print(f"ğŸ“Š {cat} å‘ç° {len(cards)} ä¸ªå¡ç‰‡", flush=True)
                
                count = 0
                for card in cards:
                    title_link = card.query_selector('a[href*="/post/"]')
                    if not title_link: continue
                    
                    clean_title = title_link.inner_text().strip()
                    href = title_link.get_attribute("href")
                    
                    raw_all = card.inner_text()
                    score_match = re.search(r'[â–²\^]\s*(\d+)', raw_all)
                    score = score_match.group(1) if score_match else "0"

                    if len(clean_title) < 5: continue

                    all_results.append({
                        "title": clean_title,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "title_cn": "",
                        "hot_info": f"ğŸ”¥{score}"
                    })
                    count += 1
                    if count >= limit: break
                page.close()
            except Exception as e:
                print(f"âŒ {cat} å¤±è´¥: {e}", flush=True)
        browser.close()
    return all_results

def main():
    print(f"â° ä»»åŠ¡å¼€å§‹æ—¶é—´: {get_beijing_time()}", flush=True)
    # å¼ºåˆ¶è·¯å¾„ï¼šç¡®ä¿åœ¨ GitHub Actions æ ¹ç›®å½•è¿è¡Œ
    data_path = Path("data.json")
    config_path = Path("config.json")

    if not config_path.exists():
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° config.json")
        return

    config = json.loads(config_path.read_text())
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 15))
    print(f"âœ… æŠ“å–å®Œæ¯•ï¼Œå…± {len(all_new)} æ¡æ•°æ®", flush=True)

    existing_data = {"items": []}
    if data_path.exists():
        try:
            existing_data = json.loads(data_path.read_text(encoding="utf-8"))
        except:
            print("âš ï¸ ç°æœ‰ data.json æŸåï¼Œå°†é‡æ–°åˆ›å»º")

    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_data.get("items", []), client)
    summary = summarize_with_ai(all_new, client)

    # åˆå¹¶å»é‡
    combined = all_new + existing_data.get("items", [])
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it)
            seen.add(it["url"])

    # å†™å…¥æ–‡ä»¶
    output = {
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }
    
    data_path.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡æˆåŠŸï¼æ–‡ä»¶å·²æ›´æ–°ã€‚å½“å‰åº“å­˜: {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
