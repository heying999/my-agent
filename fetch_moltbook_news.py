import os, re, json, time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time():
    return datetime.now(ZoneInfo("Asia/Shanghai")).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=60.0) if api_key else None

def clean_text(text: str) -> str:
    """æ·±åº¦æ¸…æ´—ï¼šå‰”é™¤æŠ•ç¥¨ç®­å¤´ã€ç‚¹èµæ•°ã€å‘å¸ƒè€…ä¿¡æ¯ï¼Œåªç•™æ ‡é¢˜"""
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    # è¿‡æ»¤è§„åˆ™ï¼šè·³è¿‡å‰å‡ è¡Œï¼ˆé€šå¸¸æ˜¯ç®­å¤´å’Œæ•°å­—ï¼‰ï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªå•è¯æ•°è¾ƒå¤šçš„è¡Œ
    for line in lines:
        if any(char in line for char in ['â–²', 'â–¼', 'Posted by']): continue
        if len(line) > 10: return line
    return lines[-1] if lines else "Untitled"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    results = []
    with sync_playwright() as p:
        print("ğŸ”¥ å¯åŠ¨é‡‡é›†å¼•æ“...", flush=True)
        browser = p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = browser.new_context(user_agent="Mozilla/5.0...", viewport={'width': 1280, 'height': 800})
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸ“¡ è®¿é—® {cat}...", flush=True)
            page = context.new_page()
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=60000)
                time.sleep(10) # å¼ºåˆ¶ç­‰å¾… AJAX
                # ç²¾å‡†å¯»æ‰¾åŒ…å« /post/ çš„é“¾æ¥
                elements = page.query_selector_all('a[href*="/post/"]')
                count, seen_urls = 0, set()
                for el in elements:
                    if count >= limit: break
                    raw_text = el.inner_text()
                    href = el.get_attribute("href")
                    title = clean_text(raw_text) # å…³é”®ï¼šè¿›å…¥æ¸…æ´—é€»è¾‘
                    
                    if len(title) < 10 or href in seen_urls: continue
                    seen_urls.add(href)
                    results.append({
                        "title": title,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "title_cn": "",
                        "hot_info": "ğŸ”¥ çƒ­é—¨"
                    })
                    count += 1
                print(f"âœ… {cat} æ•è· {count} æ¡", flush=True)
            except Exception as e: print(f"âŒ {cat} å¤±è´¥: {e}", flush=True)
            finally: page.close()
        browser.close()
    return results

def incremental_translate(items, old_items, client):
    if not client or not items: return items
    trans_map = {it["url"]: it["title_cn"] for it in old_items if it.get("title_cn") and len(it["title_cn"]) > 3}
    to_do = [it for it in items if it["url"] not in trans_map]
    
    if to_do:
        print(f"ğŸŒ ç¿»è¯‘æ–°å†…å®¹ ({len(to_do)} æ¡)...", flush=True)
        for i in range(0, len(to_do), 5):
            chunk = to_do[i:i+5]
            prompt = "è¯·ç¿»è¯‘ä»¥ä¸‹ç§‘æŠ€æ ‡é¢˜ä¸ºä¸­æ–‡ï¼Œä¸¥æ ¼ä¸€è¡Œä¸€ä¸ªï¼Œä¸¥ç¦ä»»ä½•è§£é‡Šï¼š\n\n" + "\n".join([it["title"] for it in chunk])
            try:
                res = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}]).choices[0].message.content
                lines = res.strip().splitlines()
                for j, it in enumerate(chunk):
                    it["title_cn"] = lines[j].strip() if j < len(lines) else it["title"]
            except: pass
    # å›å¡«ç¼“å­˜
    for it in items:
        if it["url"] in trans_map: it["title_cn"] = trans_map[it["url"]]
    return items

def main():
    data_path = Path("data.json")
    config = json.loads(Path("config.json").read_text())
    all_new = scrape_all_channels(config["target_urls"], config["item_limit"])
    
    old_data = json.loads(data_path.read_text(encoding="utf-8")) if data_path.exists() else {"items": []}
    client = get_ai_client()
    all_new = incremental_translate(all_new, old_data.get("items", []), client)
    
    # åˆå¹¶å»é‡å¹¶ä¿ç•™ 500 æ¡
    unique, seen = [], set()
    for it in (all_new + old_data.get("items", [])):
        if it["url"] not in seen:
            unique.append(it); seen.add(it["url"])
    
    # æ€»ç»“ç”Ÿæˆ
    summary_prompt = "è¯·ç”¨ç®€ä½“ä¸­æ–‡æ€»ç»“ä»Šæ—¥ AI 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼š10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join([it.get("title_cn") or it["title"] for it in unique[:20]])
    try:
        summary = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": summary_prompt}]).choices[0].message.content
    except: summary = "æ€»ç»“ç”Ÿæˆä¸­..."

    with open(data_path, "w", encoding="utf-8") as f:
        json.dump({"beijing_time": get_beijing_time(), "ai_summary": summary, "items": unique[:500]}, f, ensure_ascii=False, indent=2)
    print("ğŸ‰ æ•°æ®æ›´æ–°æˆåŠŸï¼", flush=True)

if __name__ == "__main__":
    main()
