#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Moltbook ç§‘æŠ€åŠ¨å‘æŠ“å–è„šæœ¬ï¼ˆå¢é‡å‚¨å­˜ç‰ˆï¼‰
åŠŸèƒ½ï¼šæŠ“å–æ•°æ®ã€AIæ€»ç»“ã€å¢é‡å‚¨å­˜è‡³ data.json å¹¶å»é‡ã€æ›´æ–° README.md
"""

import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Tuple
from urllib.parse import urljoin

from duckduckgo_search import DDGS
from playwright.sync_api import sync_playwright
from zoneinfo import ZoneInfo


def get_beijing_time() -> str:
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚"""
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M (åŒ—äº¬æ—¶é—´)")


# å¯¼èˆªç›¸å…³æ–‡æœ¬ï¼ˆæ’é™¤è¿™äº›ï¼Œé¿å…æŠŠå¯¼èˆªå½“æ ‡é¢˜ï¼‰
NAV_TEXT_BLACKLIST = {
    "login", "dashboard", "search", "loading", "moltbook", "beta", "mascot",
    "help", "developers", "privacy", "terms", "owner login", "submolts",
    "notify me", "agree", "receive emails", "built for agents",
}


def summarize_with_ddg(titles: List[str]) -> str:
    """
    ä½¿ç”¨ AI æ€»ç»“ä»Šæ—¥è¶‹åŠ¿ã€‚
    """
    if not titles:
        return ""

    def _clean_title(t: str) -> str:
        t = re.sub(r"\s+", " ", (t or "").strip())
        t = re.sub(r"^â–²\s*\d+\s*â–¼\s*Posted by\s+u/\S+\s+\S+\s+ago\s+", "", t, flags=re.I)
        t = re.sub(r"\s*ğŸ’¬\s*\d+\s*comments?\s*$", "", t, flags=re.I)
        return t.strip()

    cleaned = [_clean_title(t) for t in titles]
    cleaned = [t for t in cleaned if t]

    def _fallback_three_trends(ts: List[str]) -> str:
        # ç®€åŒ–ç‰ˆå…œåº•
        return "- **AI ä»£ç†ä¸è‡ªåŠ¨åŒ–**ï¼šè¡Œä¸šå…³æ³¨é‡ç‚¹è½¬å‘ä»£ç†æ¡†æ¶çš„ç”Ÿäº§ç¯å¢ƒè½åœ°ã€‚\n- **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šå…³äºæ™ºèƒ½ä½“é€šä¿¡åè®®ä¸å…±äº«è®°å¿†çš„è®¨è®ºå¢å¤šã€‚\n- **åŸºç¡€è®¾æ–½å»ºè®¾**ï¼šå¼€å‘è€…æ›´å€¾å‘äºåˆ©ç”¨ç»“æ„åŒ–æ•°æ®å’Œ API æ„å»ºåº•å±‚æ”¯æ’‘ã€‚"

    cleaned = cleaned[:25]
    cleaned = [t[:220] for t in cleaned]

    prompt = (
        "ä½ æ˜¯ç§‘æŠ€èµ„è®¯ç¼–è¾‘ã€‚è¯·åŸºäºä»¥ä¸‹æ ‡é¢˜åˆ—è¡¨ï¼Œç”¨ä¸­æ–‡æ€»ç»“â€œä»Šæ—¥ 3 å¤§è¶‹åŠ¿â€ã€‚\n"
        "è¦æ±‚ï¼šä¸¥æ ¼è¾“å‡º 3 æ¡ï¼›ä½¿ç”¨ Markdown æ— åºåˆ—è¡¨ï¼›æ¯æ¡ 1-2 å¥ï¼›ä¸è¦è¾“å‡ºé¢å¤–å†…å®¹ã€‚\n\n"
        "æ ‡é¢˜åˆ—è¡¨ï¼š\n" + "\n".join(f"- {t}" for t in cleaned)
    )

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    }

    try:
        raw = DDGS(headers=headers, timeout=60).chat(prompt, model="gpt-4o-mini").strip()
        if "-" in raw: return raw
        return _fallback_three_trends(cleaned)
    except:
        return _fallback_three_trends(cleaned)


def scrape_post_links_with_playwright(url: str, base_url: str, item_limit: int) -> List[Tuple[str, str]]:
    """
    æŠ“å–å¸¦ /post/ çš„é“¾æ¥ã€‚
    """
    results: List[Tuple[str, str]] = []
    seen_urls = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        try:
            page = browser.new_page()
            page.goto(url, wait_until="domcontentloaded", timeout=30000)
            page.wait_for_selector('a[href*="/post/"]', timeout=20000)
            page.wait_for_timeout(2000)

            links = page.query_selector_all('a[href*="/post/"]')
            for link in links:
                href = link.get_attribute("href")
                text = link.inner_text().strip()
                if not href or "/post/" not in href or len(text) < 2: continue
                
                if any(nav in text.lower() for nav in NAV_TEXT_BLACKLIST): continue

                full_url = urljoin(base_url, href)
                if full_url not in seen_urls:
                    seen_urls.add(full_url)
                    results.append((text, full_url))
                    if len(results) >= item_limit: break
        except Exception as e:
            print(f"æŠ“å–å‡ºé”™: {e}")
        finally:
            browser.close()
    return results


def load_config(config_path: Path) -> Tuple[str, int]:
    default_url, default_limit = "https://www.moltbook.com/m/ai", 30
    if not config_path.exists(): return default_url, default_limit
    try:
        data = json.loads(config_path.read_text(encoding="utf-8"))
        return data.get("target_url", default_url), int(data.get("item_limit", default_limit))
    except:
        return default_url, default_limit


def save_data_incremental(output_path: Path, beijing_time: str, ai_summary: str, new_items: List[Tuple[str, str]]) -> None:
    """
    æ ¸å¿ƒä¿®æ”¹ï¼šè¯»å–æ—§æ•°æ®ï¼Œåˆå¹¶ï¼Œå»é‡ï¼Œå¹¶ä¿ç•™æœ€æ–°å†…å®¹ã€‚
    """
    # 1. å°è¯•è¯»å–ç°æœ‰æ•°æ®
    existing_items = []
    if output_path.exists():
        try:
            old_data = json.loads(output_path.read_text(encoding="utf-8"))
            existing_items = old_data.get("items", [])
        except Exception as e:
            print(f"è¯»å–æ—§æ•°æ®å¤±è´¥: {e}")

    # 2. å‡†å¤‡æ–°æ•°æ®
    formatted_new = [{"title": t, "url": u} for t, u in new_items]

    # 3. åˆå¹¶å¹¶å»é‡ (ä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†)
    # é¡ºåºï¼šæ–°æŠ“å–çš„æ”¾åœ¨å‰é¢ï¼Œæ—§çš„æ”¾åœ¨åé¢
    combined_list = formatted_new + existing_items
    
    unique_items = []
    seen_urls = set()

    for item in combined_list:
        url = item.get("url")
        if url and url not in seen_urls:
            unique_items.append(item)
            seen_urls.add(url)

    # 4. æ•°é‡é™åˆ¶ï¼šä¿ç•™æœ€è¿‘ 500 æ¡ï¼Œé˜²æ­¢ JSON è¿‡å¤§
    final_items = unique_items[:500]

    # 5. ä¿å­˜
    payload = {
        "beijing_time": beijing_time,
        "ai_summary": ai_summary,  # æ€»ç»“é€šå¸¸ä¿ç•™æœ€æ–°çš„
        "items": final_items,
    }
    output_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"æ•°æ®å·²åŒæ­¥ï¼Œå½“å‰åº“å†…å…±è®¡ {len(final_items)} æ¡å»é‡è®°å½•ã€‚")


def write_readme(items: List[Tuple[str, str]], beijing_time: str, summary_md: str, output_path: Path) -> None:
    """
    README é€šå¸¸åªå±•ç¤ºå½“æ¬¡æŠ“å–çš„å†…å®¹ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥çœ‹ã€‚
    """
    lines = [
        "# ğŸ¤– Moltbook ç§‘æŠ€åŠ¨å‘è‡ªåŠ¨ç›‘æµ‹",
        "",
        f"**æ›´æ–°æ—¶é—´ï¼š** {beijing_time}",
        "",
        "## ä»Šæ—¥ 3 å¤§è¶‹åŠ¿ï¼ˆAI æ€»ç»“ï¼‰",
        "",
        summary_md if summary_md.strip() else "- ï¼ˆæš‚æ— æ€»ç»“ï¼‰",
        "",
        "## æœ€æ–°æŠ“å–åˆ—è¡¨",
        "",
    ]
    if items:
        for i, (title, url) in enumerate(items, 1):
            lines.append(f"{i}. [{title}]({url})")
    else:
        lines.append("*æš‚æ— æ–°å†…å®¹ã€‚*")
    
    output_path.write_text("\n".join(lines), encoding="utf-8")


def main() -> None:
    script_dir = Path(__file__).resolve().parent
    config_path = script_dir / "config.json"
    data_path = script_dir / "data.json"
    readme_path = script_dir / "README.md"
    
    url, item_limit = load_config(config_path)
    
    print(f"å¼€å§‹ä»»åŠ¡: {url}")
    new_items = scrape_post_links_with_playwright(url, "https://www.moltbook.com", item_limit)
    
    print("ç”Ÿæˆ AI æ€»ç»“...")
    summary = summarize_with_ddg([t for t, _ in new_items])
    
    curr_time = get_beijing_time()
    
    # æ‰§è¡Œå¢é‡ä¿å­˜
    save_data_incremental(data_path, curr_time, summary, new_items)
    # æ›´æ–° README
    write_readme(new_items, curr_time, summary, readme_path)


if __name__ == "__main__":
    main()
