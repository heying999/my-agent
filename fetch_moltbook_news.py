import os
import re
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """
    å¢é‡ç¿»è¯‘ï¼šä¿®å¤äº†å¯¹ä½é”™ä½é—®é¢˜ï¼Œé‡‡ç”¨é€æ¡æ£€æŸ¥å’Œæ›´ä¸¥æ ¼çš„ Prompt
    """
    if not client or not new_items: return new_items
    
    # å»ºç«‹ç°æœ‰ç¼“å­˜ï¼Œé¿å…é‡å¤ç¿»è¯‘
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn") and len(it["title_cn"]) > 1}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„æ‚è´¨ï¼Œé˜²æ­¢å¹²æ‰° AI
            if len(it["title"]) > 5 and not it["title"].startswith(('â–²', 'â–¼', 'Posted')):
                to_translate.append(it)
            else:
                it["title_cn"] = it["title"]

    if not to_translate: return new_items

    # æ¯æ¬¡å¤„ç†ä¸€å°æ‰¹ï¼Œç¡®ä¿å¯¹ä½å‡†ç¡®
    chunk_size = 10 
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        
        # å¼ºåˆ¶ AI æŒ‰ç…§ç‰¹å®šæ ¼å¼è¿”å›ï¼Œæ–¹ä¾¿æ­£åˆ™æ‹†åˆ†
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
            "è§„åˆ™ï¼š\n1. ä¸¥æ ¼ä¸€è¡Œå¯¹åº”ä¸€ä¸ªï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•å¤šä½™çš„è§£é‡Šæˆ–å‰ç¼€ã€‚\n"
            "2. ä¿æŒé¡ºåºä¸è¾“å…¥å®Œå…¨ä¸€è‡´ã€‚\n\n"
            + "\n".join([f"[{idx}] {it['title']}" for idx, it in enumerate(chunk)])
        )

        try:
            completion = client.chat.completions.create(
                model="qwen-plus", 
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1 # é™ä½éšæœºæ€§ï¼Œä¿è¯ç¨³å®šæ€§
            )
            raw_res = completion.choices[0].message.content.strip().splitlines()
            
            # æ¸…æ´— AI è¿”å›çš„å†…å®¹ï¼ˆå»æ‰ [0] è¿™ç§æ ‡è®°ï¼‰
            cleaned_res = [re.sub(r'^\[\d+\]\s*', '', line).strip() for line in raw_res if line.strip()]
            
            # ä¸¥æ ¼å¯¹ä½èµ‹å€¼
            for j, it in enumerate(chunk):
                if j < len(cleaned_res):
                    it["title_cn"] = cleaned_res[j]
                else:
                    it["title_cn"] = it["title"] # æ²¡ç¿»è¯‘åˆ°åˆ™ä¿ç•™åŸæ–‡
                    
        except Exception as e:
            print(f"âŒ ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: {e}")
            
    return new_items

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    
    

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0...")
        
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å–: {cat}...")
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                # ç­‰å¾…å¡ç‰‡åŠ è½½
                page.wait_for_selector('div.flex.flex-col.gap-1', timeout=15000)
                
                cards = page.query_selector_all('div.flex.flex-col.gap-1')
                count = 0
                for card in cards:
                    # --- ç²¾å‡†æå–ä¿®å¤ç‚¹ ---
                    # ä¸å†æŠ“å– card.inner_text()ï¼Œè€Œæ˜¯ç›´æ¥å®šä½æ ‡é¢˜æ‰€åœ¨çš„ <a> æ ‡ç­¾
                    title_link = card.query_selector('a[href*="/post/"]')
                    if not title_link: continue
                    
                    # è·å– A æ ‡ç­¾å†…çš„çº¯æ–‡æœ¬ï¼Œè¿™é€šå¸¸å°±æ˜¯å¹²å‡€çš„é¢˜ç›®
                    clean_title = title_link.inner_text().strip()
                    href = title_link.get_attribute("href")
                    
                    # è¾…åŠ©ï¼šè·å–çƒ­åº¦ä¿¡æ¯ç”¨äºå±•ç¤ºï¼Œä½†ä¸æ··å…¥æ ‡é¢˜
                    raw_all = card.inner_text()
                    score_match = re.search(r'[â–²\^]\s*(\d+)', raw_all)
                    comment_match = re.search(r'(\d+)\s*comments', raw_all.lower())
                    
                    score = score_match.group(1) if score_match else "0"
                    cmts = comment_match.group(1) if comment_match else "0"

                    if len(clean_title) < 5: continue

                    all_results.append({
                        "title": clean_title,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "title_cn": "", # åˆå§‹ç•™ç©º
                        "hot_info": f"ğŸ”¥{score} Â· ğŸ’¬{cmts}"
                    })
                    count += 1
                    if count >= limit: break
                page.close()
            except Exception as e:
                print(f"âŒ {cat} æŠ“å–å¤±è´¥: {e}")
        browser.close()
    return all_results

# summarize_with_ai å’Œ main ä¿æŒé€»è¾‘ï¼Œä½†ç¡®ä¿è°ƒç”¨æ›´æ–°åçš„å‡½æ•°
# ... (å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜)
