#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict  # ä¿®æ­£ï¼šè¡¥é½å¯¼å…¥ï¼Œè§£å†³ NameError
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """å¢é‡ç¿»è¯‘é€»è¾‘ï¼šåªç¿»è¯‘æ²¡æœ‰ä¸­æ–‡æ ‡é¢˜çš„æ–°æ¡ç›®"""
    if not client: return new_items
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn")}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate:
        return new_items

    max_batch = 30
    to_translate = to_translate[:max_batch]
    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(to_translate)} æ¡æ–°é¢˜ç›®...", flush=True)
    
    chunk_size = 10
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        prompt = "å°†ä»¥ä¸‹ç§‘æŠ€æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¦è¾“å‡ºç¿»è¯‘ï¼Œä¸€è¡Œä¸€ä¸ªï¼š\n\n" + "\n".join([it["title"] for it in chunk])
        try:
            completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
            res = completion.choices[0].message.content.strip().splitlines()
            for j, it in enumerate(chunk):
                if j < len(res):
                    it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[j].strip())
                else:
                    it["title_cn"] = it["title"]
        except Exception as e:
            print(f"âŒ ç¿»è¯‘æ‰¹æ¬¡å¤±è´¥: {e}", flush=True)
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return ""
    titles = [it.get("title_cn", it["title"]) for it in items[:30]]
    prompt = "æ€»ç»“ä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ã€10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(f"- {t}" for t in titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except: return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å–é¢‘é“: {cat}...", flush=True)
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=25000)
                page.wait_for_selector('a[href*="/post/"]', timeout=15000)
                
                # æŠ“å–åŒ…å«ä¿¡æ¯çš„å¡ç‰‡å®¹å™¨
                cards = page.query_selector_all('div.flex.flex-col.gap-1')
                if not cards: cards = page.query_selector_all('div.bg-slate-900') # å…¼å®¹æ¨¡å¼
                
                count = 0
                for card in cards:
                    raw_text = card.inner_text().strip()
                    link_el = card.query_selector('a[href*="/post/"]')
                    if not link_el or not raw_text: continue
                    
                    href = link_el.get_attribute("href")
                    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]
                    
                    # --- ç²¾å‡†æå–é¢˜ç›® ---
                    # å¯»æ‰¾çœŸæ­£çš„é¢˜ç›®è¡Œï¼ˆæ’é™¤ä½œè€…ã€æ—¶é—´ã€èµæ•°ç­‰åºŸè¯ï¼‰
                    clean_title = ""
                    for line in lines:
                        if any(x in line.lower() for x in ["posted by", "ago", "comments", "â–²", "^"]): continue
                        clean_title = line
                        break
                    if not clean_title: clean_title = lines[0] # å…œåº•å–ç¬¬ä¸€è¡Œ
                    
                    # --- æå–çƒ­åº¦ (Score & Comments) ---
                    score = re.search(r'[â–²\^]\s*(\d+)', raw_text)
                    comments = re.search(r'(\d+)\s*comments', raw_text.lower())
                    
                    score_val = score.group(1) if score else "0"
                    comment_val = comments.group(1) if comments else "0"
                    
                    all_results.append({
                        "title": clean_title[:200], # é™åˆ¶é•¿åº¦é˜²æ­¢ AI å‡ºé”™
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "hot_info": f"ğŸ”¥{score_val} Â· ğŸ’¬{comment_val}" # è®°å½•è¯„è®ºæ•°
                    })
                    count += 1
                    if count >= limit: break
                page.close()
            except Exception as e: print(f"âŒ {cat} æŠ“å–è¶…æ—¶: {e}", flush=True)
        browser.close()
    return all_results

def main():
    script_dir = Path(__file__).resolve().parent
    data_path = script_dir / "data.json"
    config = json.loads((script_dir / "config.json").read_text())
    
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 19))
    
    existing_items = []
    if data_path.exists():
        try: existing_items = json.loads(data_path.read_text(encoding="utf-8")).get("items", [])
        except: pass

    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it); seen.add(it["url"])

    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼Œå½“å‰åº“å­˜ {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
