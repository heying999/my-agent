#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    # å¢åŠ è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    """å¢é‡ç¿»è¯‘ï¼šåŠ å…¥å¼ºåŠ›æ—¥å¿—åˆ·æ–°å’Œå•æ¬¡ä¸Šé™æ§åˆ¶"""
    if not client: return new_items

    # å»ºç«‹ç´¢å¼•
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if "title_cn" in it}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate:
        print("â˜• æ²¡æœ‰æ–°å†…å®¹éœ€è¦ç¿»è¯‘ã€‚", flush=True)
        return new_items

    # --- æ ¸å¿ƒä¼˜åŒ–ï¼šå•æ¬¡ç¿»è¯‘ä¸Šé™ 30 æ¡ï¼Œé˜²æ­¢å¡æ­» ---
    max_batch = 30
    if len(to_translate) > max_batch:
        print(f"âš ï¸ å¾…ç¿»è¯‘é‡å¤§ ({len(to_translate)}æ¡)ï¼Œæœ¬è½®ä»…å¤„ç†å‰ {max_batch} æ¡ã€‚", flush=True)
        to_translate = to_translate[:max_batch]

    print(f"ğŸŒ å¼€å§‹ç¿»è¯‘ {len(to_translate)} æ¡æ–°å†…å®¹...", flush=True)
    
    chunk_size = 10
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        # å¼ºåˆ¶åˆ·æ–° printï¼Œè®©ä½ åœ¨ Actions å®æ—¶çœ‹åˆ°è¿›åº¦
        print(f" >> æ­£åœ¨å¤„ç†æ‰¹æ¬¡: {i+1} - {i+len(chunk)}...", flush=True)
        
        prompt = "å°†ä»¥ä¸‹ç§‘æŠ€æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¦è¾“å‡ºç¿»è¯‘ï¼Œä¸€è¡Œä¸€ä¸ªï¼š\n\n" + "\n".join([it["title"] for it in chunk])
        
        try:
            completion = client.chat.completions.create(
                model="qwen-plus",
                messages=[{"role": "user", "content": prompt}]
            )
            res = completion.choices[0].message.content.strip().splitlines()
            for j, it in enumerate(chunk):
                if j < len(res):
                    it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[j].strip())
                else:
                    it["title_cn"] = it["title"]
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤±è´¥: {e}", flush=True)
            for it in chunk: it["title_cn"] = it.get("title_cn", it["title"])
        
        time.sleep(1) # é€‚å½“åœé¡¿

    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return ""
    # æ€»ç»“ä¹Ÿåªå–æœ€è¿‘çš„ï¼Œé˜²æ­¢ Prompt è¿‡é•¿å¡æ­»
    titles = [it.get("title_cn", it["title"]) for it in items[:30]]
    prompt = "æ€»ç»“ä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ã€10æ¡åˆ—è¡¨ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(f"- {t} " for t in titles)

    try:
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}", flush=True)
        return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API çŠ¶æ€ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å– {cat}...", flush=True)
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=25000)
                page.wait_for_selector('a[href*="/post/"]', timeout=15000)
                links = page.query_selector_all('a[href*="/post/"]')
                count = 0
                for link in links:
                    href = link.get_attribute("href")
                    text = link.inner_text().strip()
                    if not href or "/post/" not in href or len(text) < 5: continue
                    all_results.append({
                        "title": text, "url": urljoin("https://www.moltbook.com", href), "category": cat
                    })
                    count += 1
                    if count >= limit: break
                page.close()
                print(f"âœ… {cat} è·å– {count} æ¡ã€‚", flush=True)
            except Exception as e:
                print(f"âŒ {cat} è¶…æ—¶è·³è¿‡ã€‚", flush=True)
        browser.close()
    return all_results

def main():
    script_dir = Path(__file__).resolve().parent
    data_path = script_dir / "data.json"
    config = json.loads((script_dir / "config.json").read_text())
    
    # 1. æŠ“å–
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 19))

    # 2. è¯»å–
    existing_items = []
    if data_path.exists():
        try: existing_items = json.loads(data_path.read_text(encoding="utf-8")).get("items", [])
        except: pass

    # 3. ç¿»è¯‘ä¸æ€»ç»“ (å¸¦ Flush æ—¥å¿—)
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    # 4. å»é‡
    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it); seen.add(it["url"])

    # 5. ä¿å­˜
    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡æˆåŠŸç»“æŸï¼Œå½“å‰åº“å­˜ {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
