#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict  # ç¡®ä¿å¯¼å…¥
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def clean_scraped_title(raw_text: str) -> str:
    """ä»æ‚ä¹±çš„å¡ç‰‡æ–‡å­—ä¸­ç²¾å‡†æå–é¢˜ç›®"""
    if not raw_text: return ""
    lines = [l.strip() for l in raw_text.splitlines() if l.strip()]
    
    # è¿‡æ»¤æ‰åŒ…å«è¿™äº›å…³é”®è¯çš„è¡Œï¼ˆç‚¹èµã€ä½œè€…ã€æ—¶é—´ã€è¯„è®ºæ•°ï¼‰
    noise_keywords = ["posted by", "ago", "comments", "â–²", "â–¼", "^"]
    
    for line in lines:
        # å¦‚æœè¿™ä¸€è¡Œä¸åŒ…å«ä»»ä½•å™ªéŸ³å…³é”®è¯ï¼Œä¸”é•¿åº¦è¶³å¤Ÿï¼Œé€šå¸¸å°±æ˜¯é¢˜ç›®
        if not any(k in line.lower() for k in noise_keywords) and len(line) > 5:
            return line
            
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å–æœ€é•¿çš„ä¸€è¡Œï¼ˆé€šå¸¸é¢˜ç›®æ¯”è¾ƒé•¿ï¼‰
    if lines:
        valid_lines = [l for l in lines if not any(k in l.lower() for k in noise_keywords)]
        if valid_lines:
            return max(valid_lines, key=len)
            
    return lines[0] if lines else ""

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    if not client or not new_items: return new_items
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn") and len(it["title_cn"]) > 1}
    
    to_translate = []
    for it in new_items:
        if it["url"] in trans_map:
            it["title_cn"] = trans_map[it["url"]]
        else:
            to_translate.append(it)
    
    if not to_translate: return new_items

    max_batch = 30
    process_list = to_translate[:max_batch]
    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(process_list)} æ¡çº¯å‡€é¢˜ç›®...", flush=True)
    
    chunk_size = 10
    for i in range(0, len(process_list), chunk_size):
        chunk = process_list[i : i + chunk_size]
        prompt = "å°†ä»¥ä¸‹ç§‘æŠ€æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¦è¾“å‡ºç¿»è¯‘ï¼Œä¸€è¡Œä¸€ä¸ªï¼š\n\n" + "\n".join([it["title"] for it in chunk])
        try:
            completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
            res = completion.choices[0].message.content.strip().splitlines()
            for j, it in enumerate(chunk):
                if j < len(res):
                    it["title_cn"] = re.sub(r'^\d+[\.ã€\s]+', '', res[j].strip())
                else: it["title_cn"] = it["title"]
        except Exception as e: print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}", flush=True)
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return ""
    # åªå–å‰ 30 æ¡ç¿»è¯‘å¥½çš„æ ‡é¢˜è¿›è¡Œæ€»ç»“
    titles = [it.get("title_cn", it["title"]) for it in items[:30] if len(it.get("title_cn", "")) > 1]
    if not titles: titles = [it["title"] for it in items[:15]] # å…œåº•

    prompt = "æ€»ç»“ä»Šæ—¥ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ã€10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(f"- {t}" for t in titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except: return "- ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸš€ æ­£åœ¨æŠ“å–: {cat}...", flush=True)
            try:
                page = context.new_page()
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                page.wait_for_selector('a[href*="/post/"]', timeout=20000)
                
                cards = page.query_selector_all('div.flex.flex-col.gap-1')
                count = 0
                for card in cards:
                    raw_text = card.inner_text().strip()
                    link_el = card.query_selector('a[href*="/post/"]')
                    if not link_el or not raw_text: continue
                    
                    href = link_el.get_attribute("href")
                    # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šè°ƒç”¨æ¸…æ´—å‡½æ•°æå–çœŸæ­£çš„é¢˜ç›®
                    clean_title = clean_scraped_title(raw_text)
                    
                    # æå–çƒ­åº¦ä¿¡æ¯
                    score = re.search(r'[â–²\^]\s*(\d+)', raw_text)
                    comments = re.search(r'(\d+)\s*comments', raw_text.lower())
                    score_val = score.group(1) if score else "0"
                    comment_val = comments.group(1) if comments else "0"

                    if len(clean_title) < 5: continue

                    all_results.append({
                        "title": clean_title,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "hot_info": f"ğŸ”¥{score_val} Â· ğŸ’¬{comment_val}"
                    })
                    count += 1
                    if count >= limit: break
                page.close()
            except Exception as e: print(f"âŒ {cat} é”™è¯¯: {e}", flush=True)
        browser.close()
    return all_results

def main():
    script_dir = Path(__file__).resolve().parent
    data_path = script_dir / "data.json"
    config = json.loads((script_dir / "config.json").read_text())
    
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 19))
    
    existing_items = []
    if data_path.exists():
        try: existing_items = json.loads(data_path.read_text(encoding="utf-8")).get("items", [])
        except: pass

    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new, client)

    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it); seen.add(it["url"])

    data_path.write_text(json.dumps({
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ æˆåŠŸï¼å½“å‰åº“å­˜ {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
