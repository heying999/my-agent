import os
import re
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from urllib.parse import urljoin
from openai import OpenAI
from zoneinfo import ZoneInfo

def get_beijing_time() -> str:
    tz = ZoneInfo("Asia/Shanghai")
    return datetime.now(tz).strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

def get_ai_client():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key: 
        print("âš ï¸ ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®")
        return None
    return OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1", timeout=30.0)

def incremental_translate(new_items: List[Dict], existing_items: List[Dict], client: OpenAI) -> List[Dict]:
    if not client or not new_items: return new_items
    trans_map = {it["url"]: it["title_cn"] for it in existing_items if it.get("title_cn") and len(it["title_cn"]) > 1}
    to_translate = [it for it in new_items if it["url"] not in trans_map]

    if not to_translate: 
        print("âœ… æ— éœ€ç¿»è¯‘æ–°æ¡ç›®ã€‚")
        # è¡¥å…¨å·²æœ‰ç¿»è¯‘
        for it in new_items: it["title_cn"] = trans_map.get(it["url"], "")
        return new_items

    print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ {len(to_translate)} æ¡æ–°é¢˜ç›®...", flush=True)
    chunk_size = 10 
    for i in range(0, len(to_translate), chunk_size):
        chunk = to_translate[i : i + chunk_size]
        prompt = (
            "ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
            "è§„åˆ™ï¼šä¸¥æ ¼ä¸€è¡Œå¯¹åº”ä¸€ä¸ªï¼Œä¿æŒé¡ºåºï¼Œä¸¥ç¦è§£é‡Šã€‚\n\n"
            + "\n".join([f"[{idx}] {it['title']}" for idx, it in enumerate(chunk)])
        )
        try:
            completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}], temperature=0.1)
            res = completion.choices[0].message.content.strip().splitlines()
            cleaned = [re.sub(r'^\[\d+\]\s*', '', l).strip() for l in res if l.strip()]
            for j, it in enumerate(chunk):
                it["title_cn"] = cleaned[j] if j < len(cleaned) else it["title"]
        except Exception as e:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            for it in chunk: it["title_cn"] = it["title"]
    return new_items

def summarize_with_ai(items: List[Dict], client: OpenAI) -> str:
    if not client or not items: return "æš‚æ— æ‘˜è¦"
    titles = [it.get("title_cn") or it["title"] for it in items[:25]]
    prompt = "ç”¨ç®€ä½“ä¸­æ–‡æ€»ç»“ä»Šæ—¥ AI/ç§‘æŠ€ 10 å¤§æ ¸å¿ƒåŠ¨å‘ã€‚10æ¡ã€åŠ ç²—å…³é”®è¯ã€ä¸¥ç¦è‹±æ–‡ã€‚\n\n" + "\n".join(titles)
    try:
        completion = client.chat.completions.create(model="qwen-plus", messages=[{"role": "user", "content": prompt}])
        return completion.choices[0].message.content.strip()
    except: return "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼‰"

def scrape_all_channels(urls: List[str], limit: int) -> List[Dict]:
    from playwright.sync_api import sync_playwright
    all_results = []
    
    with sync_playwright() as p:
        print("ğŸ”¥ å¯åŠ¨å¢å¼ºå‹æµè§ˆå™¨...", flush=True)
        # å¢åŠ å‚æ•°æé«˜åœ¨ Linux CI ç¯å¢ƒä¸‹çš„ç¨³å®šæ€§
        browser = p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu'])
        
        # æ¨¡æ‹ŸçœŸå®è®¾å¤‡è§†å£ï¼Œé˜²æ­¢æŸäº›å“åº”å¼é¡µé¢ä¸æ¸²æŸ“å†…å®¹
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1280, 'height': 800}
        )
        
        for url in urls:
            cat = url.split('/')[-1].upper()
            print(f"ğŸ“¡ è®¿é—® {cat}: {url}", flush=True)
            page = context.new_page()
            try:
                # ç­–ç•¥ 1: å¢åŠ è¶…æ—¶åˆ° 60sï¼Œç­‰å¾…ç½‘ç»œç©ºé—² (networkidle)
                page.goto(url, wait_until="networkidle", timeout=60000)
                
                # ç­–ç•¥ 2: é¢å¤–æ»šåŠ¨ä¸€ä¸‹ï¼Œè§¦å‘æ‡’åŠ è½½
                page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
                
                # ç­–ç•¥ 3: ç­‰å¾…ç‰¹å®šé€‰æ‹©å™¨ï¼Œå¢åŠ å®¹é”™
                try:
                    page.wait_for_selector('div.flex.flex-col.gap-1', timeout=30000)
                except:
                    print(f"âš ï¸ {cat} è¶…æ—¶æœªè§æ ‡å‡†å¡ç‰‡ï¼Œå°è¯•è¯»å–é¡µé¢æ ‡é¢˜: {page.title()}")
                
                cards = page.query_selector_all('div.flex.flex-col.gap-1')
                print(f"ğŸ“Š {cat} å‘ç° {len(cards)} ä¸ªå¡ç‰‡", flush=True)
                
                count = 0
                for card in cards:
                    title_link = card.query_selector('a[href*="/post/"]')
                    if not title_link: continue
                    
                    clean_title = title_link.inner_text().strip()
                    href = title_link.get_attribute("href")
                    
                    # ç®€å•çƒ­åº¦æŠ“å–
                    raw_all = card.inner_text()
                    score = re.search(r'[â–²\^]\s*(\d+)', raw_all)
                    score_val = score.group(1) if score else "0"

                    if len(clean_title) < 5: continue

                    all_results.append({
                        "title": clean_title,
                        "url": urljoin("https://www.moltbook.com", href),
                        "category": cat,
                        "title_cn": "",
                        "hot_info": f"ğŸ”¥{score_val}"
                    })
                    count += 1
                    if count >= limit: break
            except Exception as e:
                print(f"âŒ {cat} é¢‘é“æŠ“å–ä¸­æ–­: {e}", flush=True)
            finally:
                page.close()
        browser.close()
    return all_results

def main():
    print(f"ğŸ¬ æœºå™¨äººå¯åŠ¨ | åŒ—äº¬æ—¶é—´: {get_beijing_time()}", flush=True)
    data_path = Path("data.json")
    config_path = Path("config.json")

    if not config_path.exists():
        print("âŒ é”™è¯¯: æ ¹ç›®å½•ç¼ºå°‘ config.json")
        return

    config = json.loads(config_path.read_text())
    # 1. æŠ“å–
    all_new = scrape_all_channels(config.get("target_urls", []), config.get("item_limit", 15))
    
    if not all_new:
        print("âš ï¸ æœ¬æ¬¡è¿è¡ŒæœªæŠ“å–åˆ°ä»»ä½•æ–°æ•°æ®ï¼Œå¯èƒ½ç”±äºç½‘ç»œè¶…æ—¶ã€‚")

    # 2. è¯»å–æ—§æ•°æ®
    existing_items = []
    if data_path.exists():
        try:
            old_data = json.loads(data_path.read_text(encoding="utf-8"))
            existing_items = old_data.get("items", [])
        except: pass

    # 3. ç¿»è¯‘ & æ€»ç»“
    client = get_ai_client()
    all_new = incremental_translate(all_new, existing_items, client)
    summary = summarize_with_ai(all_new + existing_items, client)

    # 4. åˆå¹¶å»é‡ (ä»¥ URL ä¸ºå‡†)
    combined = all_new + existing_items
    unique, seen = [], set()
    for it in combined:
        if it["url"] not in seen:
            unique.append(it)
            seen.add(it["url"])

    # 5. å†™å…¥
    output = {
        "beijing_time": get_beijing_time(),
        "ai_summary": summary,
        "items": unique[:500]
    }
    
    data_path.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ğŸ‰ ä»»åŠ¡ç»“æŸï¼å½“å‰åº“å­˜: {len(unique[:500])} æ¡ã€‚", flush=True)

if __name__ == "__main__":
    main()
